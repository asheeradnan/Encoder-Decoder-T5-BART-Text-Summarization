# Encoder-Decoder-T5-BART-Text-Summarization
Generate concise summaries of long documents or articles using an Encoder–Decoder architecture.
<img width="1919" height="1036" alt="Screenshot 2025-11-12 232446" src="https://github.com/user-attachments/assets/51a022df-8df1-49b8-b628-c1c8c7baed6b" />

Dataset: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-
cnn-dailymail

Objective: Fine-tune an Encoder–Decoder model to perform abstractive text
summarization of input passages.


Deliverables:
 Dataset preprocessing (article to summary pairs)
 Model fine-tuning code using Hugging Face Transformers
 Evaluation metrics: ROUGE-1, ROUGE-2, ROUGE-L, and qualitative comparison
 Example outputs with original text and generated summary


(NLP_assign-3_task-3)

